{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc4ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from transformers import LukeTokenizer, LukeForEntitySpanClassification\n",
    "import timeit\n",
    "import ast\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import seqeval.metrics\n",
    "import spacy\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4fab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the testb set of the CoNLL-2003 dataset\n",
    "!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04664c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LukeForEntitySpanClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eaf1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(dataset_file):\n",
    "    documents = []\n",
    "    words = []\n",
    "    labels = []\n",
    "    sentence_boundaries = []\n",
    "    with open(dataset_file) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith(\"-DOCSTART\"):\n",
    "                if words:\n",
    "                    documents.append(dict(\n",
    "                        words=words,\n",
    "                        labels=labels,\n",
    "                        sentence_boundaries=sentence_boundaries\n",
    "                    ))\n",
    "                    words = []\n",
    "                    labels = []\n",
    "                    sentence_boundaries = []\n",
    "                continue\n",
    "\n",
    "            if not line:\n",
    "                if not sentence_boundaries or len(words) != sentence_boundaries[-1]:\n",
    "                    sentence_boundaries.append(len(words))\n",
    "            else:\n",
    "                items = line.split(\" \")\n",
    "                words.append(items[0])\n",
    "                labels.append(items[-1])\n",
    "\n",
    "    if words:\n",
    "        documents.append(dict(\n",
    "            words=words,\n",
    "            labels=labels,\n",
    "            sentence_boundaries=sentence_boundaries\n",
    "        ))\n",
    "        \n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_examples(documents):\n",
    "    examples = []\n",
    "    max_token_length = 510\n",
    "    max_mention_length = 30\n",
    "\n",
    "    for document in tqdm(documents):\n",
    "        words = document[\"words\"]\n",
    "        subword_lengths = [len(tokenizer.tokenize(w)) for w in words]\n",
    "        total_subword_length = sum(subword_lengths)\n",
    "        sentence_boundaries = document[\"sentence_boundaries\"]\n",
    "\n",
    "        for i in range(len(sentence_boundaries) - 1):\n",
    "            sentence_start, sentence_end = sentence_boundaries[i:i+2]\n",
    "            if total_subword_length <= max_token_length:\n",
    "                # if the total sequence length of the document is shorter than the\n",
    "                # maximum token length, we simply use all words to build the sequence\n",
    "                context_start = 0\n",
    "                context_end = len(words)\n",
    "            else:\n",
    "                # if the total sequence length is longer than the maximum length, we add\n",
    "                # the surrounding words of the target sentenceã€€to the sequence until it\n",
    "                # reaches the maximum length\n",
    "                context_start = sentence_start\n",
    "                context_end = sentence_end\n",
    "                cur_length = sum(subword_lengths[context_start:context_end])\n",
    "                while True:\n",
    "                    if context_start > 0:\n",
    "                        if cur_length + subword_lengths[context_start - 1] <= max_token_length:\n",
    "                            cur_length += subword_lengths[context_start - 1]\n",
    "                            context_start -= 1\n",
    "                        else:\n",
    "                            break\n",
    "                    if context_end < len(words):\n",
    "                        if cur_length + subword_lengths[context_end] <= max_token_length:\n",
    "                            cur_length += subword_lengths[context_end]\n",
    "                            context_end += 1\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "            text = \"\"\n",
    "            for word in words[context_start:sentence_start]:\n",
    "                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n",
    "                    text = text.rstrip()\n",
    "                text += word\n",
    "                text += \" \"\n",
    "\n",
    "            sentence_words = words[sentence_start:sentence_end]\n",
    "            sentence_subword_lengths = subword_lengths[sentence_start:sentence_end]\n",
    "\n",
    "            word_start_char_positions = []\n",
    "            word_end_char_positions = []\n",
    "            for word in sentence_words:\n",
    "                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n",
    "                    text = text.rstrip()\n",
    "                word_start_char_positions.append(len(text))\n",
    "                text += word\n",
    "                word_end_char_positions.append(len(text))\n",
    "                text += \" \"\n",
    "\n",
    "            for word in words[sentence_end:context_end]:\n",
    "                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n",
    "                    text = text.rstrip()\n",
    "                text += word\n",
    "                text += \" \"\n",
    "            text = text.rstrip()\n",
    "\n",
    "            entity_spans = []\n",
    "            original_word_spans = []\n",
    "            for word_start in range(len(sentence_words)):\n",
    "                for word_end in range(word_start, len(sentence_words)):\n",
    "                    if sum(sentence_subword_lengths[word_start:word_end]) <= max_mention_length:\n",
    "                        entity_spans.append(\n",
    "                            (word_start_char_positions[word_start], word_end_char_positions[word_end])\n",
    "                        )\n",
    "                        original_word_spans.append(\n",
    "                            (word_start, word_end + 1)\n",
    "                        )\n",
    "\n",
    "            examples.append(dict(\n",
    "                text=text,\n",
    "                words=sentence_words,\n",
    "                entity_spans=entity_spans,\n",
    "                original_word_spans=original_word_spans,\n",
    "            ))\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def is_punctuation(char):\n",
    "    cp = ord(char)\n",
    "    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d2e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents = load_documents(\"eng.testb\")\n",
    "test_examples = load_examples(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a66891",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "all_logits = []\n",
    "\n",
    "for batch_start_idx in trange(0, len(test_examples), batch_size):\n",
    "    batch_examples = test_examples[batch_start_idx:batch_start_idx + batch_size]\n",
    "    texts = [example[\"text\"] for example in batch_examples]\n",
    "    entity_spans = [example[\"entity_spans\"] for example in batch_examples]\n",
    "\n",
    "    inputs = tokenizer(texts, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    all_logits.extend(outputs.logits.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels = [label for document in test_documents for label in document[\"labels\"]]\n",
    "final_predictions = []\n",
    "for example_index, example in enumerate(test_examples):\n",
    "    logits = all_logits[example_index]\n",
    "    max_logits = np.max(logits, axis=1)\n",
    "    max_indices = np.argmax(logits, axis=1)\n",
    "    original_spans = example[\"original_word_spans\"]\n",
    "    predictions = []\n",
    "    for logit, index, span in zip(max_logits, max_indices, original_spans):\n",
    "        if index != 0:  # the span is not NIL\n",
    "            predictions.append((logit, span, model.config.id2label[index]))\n",
    "\n",
    "    # construct an IOB2 label sequence\n",
    "    predicted_sequence = [\"O\"] * len(example[\"words\"])\n",
    "    for _, span, label in sorted(predictions, key=lambda o: o[0], reverse=True):\n",
    "        if all([o == \"O\" for o in predicted_sequence[span[0] : span[1]]]):\n",
    "            predicted_sequence[span[0]] = \"B-\" + label\n",
    "            if span[1] - span[0] > 1:\n",
    "                predicted_sequence[span[0] + 1 : span[1]] = [\"I-\" + label] * (span[1] - span[0] - 1)\n",
    "\n",
    "    final_predictions += predicted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seqeval.metrics.classification_report([final_labels], [final_predictions], digits=4)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
