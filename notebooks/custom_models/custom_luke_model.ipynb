{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nerda -q\n",
    "!pip install seqeval -q\n",
    "!pip install flair -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NERDA.datasets import get_conll_data, download_conll_data \n",
    "from NERDA.models import NERDA\n",
    "\n",
    "from google.colab import files\n",
    "import pandas as pd\n",
    "import ast\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import seqeval.metrics\n",
    "import spacy\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import LukeTokenizer, LukeForEntitySpanClassification\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# must upload processed_df.csv and retrain_processed.csv files\n",
    "uploaded = files.upload()\n",
    "\n",
    "\n",
    "download_conll_data()\n",
    "training = get_conll_data('train')\n",
    "validation = get_conll_data('valid')\n",
    "testing = get_conll_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626dbdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the testb set of the CoNLL-2003 dataset\n",
    "!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(input_text):\n",
    "  input_text = str(input_text)\n",
    "  if input_text.count(\" \") > 0:\n",
    "    if \"went to the store\" in input_text:\n",
    "      if input_text.count(\" \") > 4:\n",
    "        return [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\"]\n",
    "      return [\"B-PER\", \"O\", \"O\", \"O\", \"O\"]\n",
    "    return [\"B-PER\", \"I-PER\"]\n",
    "  else: \n",
    "    return [\"B-PER\"]\n",
    "\n",
    "def get_sentence_from_name(input_name):\n",
    "  input_name = str(input_name)\n",
    "  return input_name.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_scheme = [\n",
    "'B-PER',\n",
    "'I-PER',\n",
    "'B-ORG',\n",
    "'I-ORG',\n",
    "'B-LOC',\n",
    "'I-LOC',\n",
    "'B-MISC',\n",
    "'I-MISC'\n",
    "]\n",
    "\n",
    "transformer = 'studio-ousia/luke-large-finetuned-conll-2003'\n",
    "\n",
    "# hyperparameters for network\n",
    "dropout = 0.1\n",
    "\n",
    "training_hyperparameters = {\n",
    "'epochs' : 2,\n",
    "'warmup_steps' : 500,                                                   \n",
    "'train_batch_size': 13,                                         \n",
    "'learning_rate': 1e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_subset = pd.read_csv(\"retrain_processed.csv\", index_col=0)\n",
    "retrain_subset[\"tags_list\"] = retrain_subset[\"Name\"].apply(lambda x: generate_labels(x))\n",
    "retrain_subset[\"sentences\"] = retrain_subset[\"Name\"].apply(lambda x: get_sentence_from_name(x))\n",
    "\n",
    "rt_train, rt_valid = train_test_split(retrain_subset, test_size=0.15, stratify=retrain_subset['Race']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_subset = pd.read_csv(\"retrain_processed.csv\", index_col=0)\n",
    "retrain_subset[\"tags_list\"] = retrain_subset[\"Name\"].apply(lambda x: generate_labels(x))\n",
    "retrain_subset[\"sentences\"] = retrain_subset[\"Name\"].apply(lambda x: get_sentence_from_name(x))b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_dict = {\"sentences\": list(rt_train[\"sentences\"]), \"tags\": list(rt_train[\"tags_list\"])}\n",
    "valid_dict = {\"sentences\": list(rt_valid[\"sentences\"]), \"tags\": list(rt_valid[\"tags_list\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20fd7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentences = list(retrain_dict[\"sentences\"]) + list(training[\"sentences\"])\n",
    "total_tags = list(retrain_dict[\"tags\"]) + list(training[\"tags\"])\n",
    "\n",
    "valid_sentences = list(valid_dict[\"sentences\"]) + list(validation[\"sentences\"])\n",
    "valid_tags = list(valid_dict[\"tags\"]) + list(validation[\"tags\"])\n",
    "\n",
    "total_retrain_dict = {\"sentences\": total_sentences, \"tags\": total_tags}\n",
    "total_valid_dict = {\"sentences\": valid_sentences, \"tags\": valid_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NERDA(\n",
    "dataset_training = total_retrain_dict,\n",
    "dataset_validation = total_valid_dict,\n",
    "tag_scheme = tag_scheme, \n",
    "tag_outside = 'O',\n",
    "transformer = transformer,\n",
    "dropout = dropout,\n",
    "hyperparameters = training_hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_df = pd.read_csv(\"processed_df.csv\", index_col=0)\n",
    "processed_test_df[\"tags_list\"] = processed_test_df[\"Name\"].apply(lambda x: generate_labels(x))\n",
    "processed_test_df[\"sentences\"] = processed_test_df[\"Name\"].apply(lambda x: get_sentence_from_name(x))\n",
    "processed_test_dict = {\"sentences\": list(processed_test_df[\"sentences\"]), \"tags\": list(processed_test_df[\"tags_list\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entities(input_string):\n",
    "  return model.predict([input_string])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326cbb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_white_df = processed_test_df.loc[processed_test_df[\"Race\"]==\"White\"].reset_index(drop=True)\n",
    "processed_black_df = processed_test_df.loc[processed_test_df[\"Race\"]==\"Black\"].reset_index(drop=True)\n",
    "processed_api_df = processed_test_df.loc[processed_test_df[\"Race\"]==\"API\"].reset_index(drop=True)\n",
    "processed_hispanic_df = processed_test_df.loc[processed_test_df[\"Race\"]==\"Hispanic\"].reset_index(drop=True)\n",
    "\n",
    "processed_test_dict_w = {\"sentences\": list(processed_white_df[\"sentences\"]), \"tags\": list(processed_white_df[\"tags_list\"])}\n",
    "processed_test_dict_b = {\"sentences\": list(processed_black_df[\"sentences\"]), \"tags\": list(processed_black_df[\"tags_list\"])}\n",
    "processed_test_dict_a = {\"sentences\": list(processed_api_df[\"sentences\"]), \"tags\": list(processed_api_df[\"tags_list\"])}\n",
    "processed_test_dict_h = {\"sentences\": list(processed_hispanic_df[\"sentences\"]), \"tags\": list(processed_hispanic_df[\"tags_list\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6afd74b",
   "metadata": {},
   "source": [
    "## Primarily White Names from Curated Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2aa1e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8_/p6s91lnx1vn9_y920h68jhy00000gn/T/ipykernel_5703/166549778.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcurated_test_labels_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_test_dict_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tags\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcurated_pred_labels_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_test_dict_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentences\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_test_dict_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentences\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurated_test_labels_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurated_pred_labels_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "curated_test_labels_w = processed_test_dict_w[\"tags\"]\n",
    "curated_pred_labels_w = [generate_entities(processed_test_dict_w[\"sentences\"][i]) for i in range(len(processed_test_dict_w[\"sentences\"]))]\n",
    "print(seqeval.metrics.classification_report(curated_test_labels_w, curated_pred_labels_w, digits=4)) \n",
    "stop = timeit.default_timer()\n",
    "print('LUKE Runtime: {} seconds'.format(stop - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc84ae0",
   "metadata": {},
   "source": [
    "## Primarily Black / African American Names from Curated Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b80d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "curated_test_labels_b = processed_test_dict_b[\"tags\"]\n",
    "curated_pred_labels_b = [generate_entities(processed_test_dict_b[\"sentences\"][i]) for i in range(len(processed_test_dict_b[\"sentences\"]))]\n",
    "print(seqeval.metrics.classification_report(curated_test_labels_b, curated_pred_labels_b, digits=4)) \n",
    "stop = timeit.default_timer()\n",
    "print('LUKE Runtime: {} seconds'.format(stop - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979f962",
   "metadata": {},
   "source": [
    "## Primarily Asian or Native Hawaiian or Other Pacific Islander Names from Curated Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf0d3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "curated_test_labels_a = processed_test_dict_a[\"tags\"]\n",
    "curated_pred_labels_a = [generate_entities(processed_test_dict_a[\"sentences\"][i]) for i in range(len(processed_test_dict_a[\"sentences\"]))]\n",
    "print(seqeval.metrics.classification_report(curated_test_labels_a, curated_pred_labels_a, digits=4)) \n",
    "stop = timeit.default_timer()\n",
    "print('LUKE Runtime: {} seconds'.format(stop - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9adc64",
   "metadata": {},
   "source": [
    "## Primarily Hispanic / Latino Names from Curated Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b84e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "curated_test_labels_h = processed_test_dict_h[\"tags\"]\n",
    "curated_pred_labels_h = [generate_entities(processed_test_dict_h[\"sentences\"][i]) for i in range(len(processed_test_dict_h[\"sentences\"]))]\n",
    "print(seqeval.metrics.classification_report(curated_test_labels_h, curated_pred_labels_h, digits=4)) \n",
    "stop = timeit.default_timer()\n",
    "print('LUKE Runtime: {} seconds'.format(stop - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n",
    "\n",
    "def load_documents(dataset_file):\n",
    "    documents = []\n",
    "    words = []\n",
    "    labels = []\n",
    "    sentence_boundaries = []\n",
    "    with open(dataset_file) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith(\"-DOCSTART\"):\n",
    "                if words:\n",
    "                    documents.append(dict(\n",
    "                        words=words,\n",
    "                        labels=labels,\n",
    "                        sentence_boundaries=sentence_boundaries\n",
    "                    ))\n",
    "                    words = []\n",
    "                    labels = []\n",
    "                    sentence_boundaries = []\n",
    "                continue\n",
    "\n",
    "            if not line:\n",
    "                if not sentence_boundaries or len(words) != sentence_boundaries[-1]:\n",
    "                    sentence_boundaries.append(len(words))\n",
    "            else:\n",
    "                items = line.split(\" \")\n",
    "                words.append(items[0])\n",
    "                labels.append(items[-1])\n",
    "\n",
    "    if words:\n",
    "        documents.append(dict(\n",
    "            words=words,\n",
    "            labels=labels,\n",
    "            sentence_boundaries=sentence_boundaries\n",
    "        ))\n",
    "        \n",
    "    return documents\n",
    "\n",
    "\n",
    "def load_examples(documents):\n",
    "    examples = []\n",
    "    max_token_length = 510\n",
    "    max_mention_length = 30\n",
    "\n",
    "    for document in tqdm(documents):\n",
    "        words = document[\"words\"]\n",
    "        subword_lengths = [len(tokenizer.tokenize(w)) for w in words]\n",
    "        total_subword_length = sum(subword_lengths)\n",
    "        sentence_boundaries = document[\"sentence_boundaries\"]\n",
    "\n",
    "        for i in range(len(sentence_boundaries) - 1):\n",
    "            sentence_start, sentence_end = sentence_boundaries[i:i+2]\n",
    "            if total_subword_length <= max_token_length:\n",
    "                # if the total sequence length of the document is shorter than the\n",
    "                # maximum token length, we simply use all words to build the sequence\n",
    "                context_start = 0\n",
    "                context_end = len(words)\n",
    "            else:\n",
    "                # if the total sequence length is longer than the maximum length, we add\n",
    "                # the surrounding words of the target sentence　to the sequence until it\n",
    "                # reaches the maximum length\n",
    "                context_start = sentence_start\n",
    "                context_end = sentence_end\n",
    "                cur_length = sum(subword_lengths[context_start:context_end])\n",
    "                while True:\n",
    "                    if context_start > 0:\n",
    "                        if cur_length + subword_lengths[context_start - 1] <= max_token_length:\n",
    "                            cur_length += subword_lengths[context_start - 1]\n",
    "                            context_start -= 1\n",
    "                        else:\n",
    "                            break\n",
    "                    if context_end < len(words):\n",
    "                        if cur_length + subword_lengths[context_end] <= max_token_length:\n",
    "                            cur_length += subword_lengths[context_end]\n",
    "                            context_end += 1\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "            text = \"\"\n",
    "            for word in words[context_start:sentence_start]:\n",
    "                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n",
    "                    text = text.rstrip()\n",
    "                text += word\n",
    "                text += \" \"\n",
    "\n",
    "            sentence_words = words[sentence_start:sentence_end]\n",
    "            sentence_subword_lengths = subword_lengths[sentence_start:sentence_end]\n",
    "\n",
    "            word_start_char_positions = []\n",
    "            word_end_char_positions = []\n",
    "            for word in sentence_words:\n",
    "                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n",
    "                    text = text.rstrip()\n",
    "                word_start_char_positions.append(len(text))\n",
    "                text += word\n",
    "                word_end_char_positions.append(len(text))\n",
    "                text += \" \"\n",
    "\n",
    "            for word in words[sentence_end:context_end]:\n",
    "                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n",
    "                    text = text.rstrip()\n",
    "                text += word\n",
    "                text += \" \"\n",
    "            text = text.rstrip()\n",
    "\n",
    "            entity_spans = []\n",
    "            original_word_spans = []\n",
    "            for word_start in range(len(sentence_words)):\n",
    "                for word_end in range(word_start, len(sentence_words)):\n",
    "                    if sum(sentence_subword_lengths[word_start:word_end]) <= max_mention_length:\n",
    "                        entity_spans.append(\n",
    "                            (word_start_char_positions[word_start], word_end_char_positions[word_end])\n",
    "                        )\n",
    "                        original_word_spans.append(\n",
    "                            (word_start, word_end + 1)\n",
    "                        )\n",
    "\n",
    "            examples.append(dict(\n",
    "                text=text,\n",
    "                words=sentence_words,\n",
    "                entity_spans=entity_spans,\n",
    "                original_word_spans=original_word_spans,\n",
    "            ))\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def is_punctuation(char):\n",
    "    cp = ord(char)\n",
    "    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n",
    "        return True\n",
    "    cat = unicodedata.category(char)\n",
    "    if cat.startswith(\"P\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8487aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents = load_documents(\"eng.testb\")\n",
    "test_examples = load_examples(test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075dde16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_entities_custom_luke(input_row):\n",
    "  words = input_row[\"words\"]\n",
    "  sentence_boundaries = input_row[\"sentence_boundaries\"]\n",
    "  start = 0\n",
    "  total_labels = []\n",
    "  for i in sentence_boundaries: \n",
    "    if i != 0: \n",
    "      current_string = words[start:i]\n",
    "      if len(current_string) >= 120:\n",
    "        midpoint = len(current_string) // 2\n",
    "        first_half = current_string[:midpoint]\n",
    "        second_half = current_string[midpoint:]\n",
    "        prediction = model.predict([first_half])[0] + model.predict([second_half])[0]\n",
    "      else: \n",
    "        prediction = model.predict([current_string])[0]\n",
    "      total_labels.extend(prediction)\n",
    "      start = i\n",
    "  return total_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04287ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = [test_documents[i][\"labels\"] for i in range(len(test_documents))]\n",
    "pred_labels = [get_named_entities_custom_luke(test_documents[i]) for i in range(len(test_documents))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seqeval.metrics.classification_report(test_labels, pred_labels, digits=4)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
